{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"F:/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers, activations\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras import models\n",
    "from keras.layers.core import Activation, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/data/train_images/\n",
      "['F:/data/1/', 'F:/data/2/', 'F:/data/3/', 'F:/data/4/']\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 8\n",
    "val_batch_size = 8\n",
    "\n",
    "IMAGE_SIZE_WIDTH = 1600\n",
    "IMAGE_SIZE_HEIGHT = 256\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "train_image_dir = root_dir + \"train_images/\"\n",
    "test_image_dirs = [root_dir + str(i)+'/' for i in range(1,5)]\n",
    "print(train_image_dir)\n",
    "print(test_image_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "  \"\"\" Load an image from disk. \"\"\"\n",
    "  return mpimg.imread(infilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7ce5424717e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;31m#len(files)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" images from\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_dir' is not defined"
     ]
    }
   ],
   "source": [
    "files = os.listdir(image_dir)\n",
    "n = 10#len(files)\n",
    "\n",
    "print(\"Loading \" + str(n) + \" images from\",image_dir)\n",
    "\n",
    "tt=[]\n",
    "\n",
    "fail=0\n",
    "for i in range(n):\n",
    "    try:\n",
    "        tt.append(load_image(train_image_dir + files[i]))\n",
    "        print(files[i])\n",
    "        sys.stdout.write(\"\\033[F\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed at\",train_image_dir,files[i])\n",
    "        print(e)\n",
    "print(len(tt),\"Train images Loaded\")\n",
    "\n",
    "X = np.asarray(tt)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_images(gt_dir,n):\n",
    "    img_data_list = [] \n",
    "    labels_list = []\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    img_data_list = [] \n",
    "    labels_list = []\n",
    "    for i in range(n):\n",
    "        input_img=cv2.imread(gt_dir + files[i])\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        #mask = img_to_array(load_img(path_train + '//masks//' + id_, grayscale=False))\n",
    "        input_img = resize(input_img, (256,1600,3), mode='constant', preserve_range=True)\n",
    "        #input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img)\n",
    "        #labels_list.append(label)\n",
    "    \n",
    "    y = np.array(img_data_list)\n",
    "    y = y.astype('float32')\n",
    "    y /= 255\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    precision_n = precision(y_true, y_pred)\n",
    "    recall_n = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_n * recall_n) / (precision_n + recall_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers.core import Activation, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import json\n",
    "\n",
    "img_w = IMAGE_SIZE_WIDTH\n",
    "img_h = IMAGE_SIZE_HEIGHT\n",
    "n_labels = 4\n",
    "\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    kernel = 3\n",
    "\n",
    "    encoding_layers = [\n",
    "        Convolution2D(64, kernel, border_mode='same', input_shape=( img_h, img_w,3)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(64, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "    ]\n",
    "\n",
    "    autoencoder = models.Sequential()\n",
    "    autoencoder.encoding_layers = encoding_layers\n",
    "\n",
    "    for l in autoencoder.encoding_layers:\n",
    "        autoencoder.add(l)\n",
    "        print(l.input_shape,l.output_shape,l)\n",
    "\n",
    "    decoding_layers = [\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(512, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(256, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(128, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Convolution2D(64, kernel, kernel, border_mode='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Convolution2D(n_labels, 1, 1, border_mode='valid'),\n",
    "        BatchNormalization(),\n",
    "    ]\n",
    "    autoencoder.decoding_layers = decoding_layers\n",
    "    for l in autoencoder.decoding_layers:\n",
    "        autoencoder.add(l)\n",
    "\n",
    "    with open('model_5l.json', 'w') as outfile:\n",
    "        outfile.write(json.dumps(json.loads(autoencoder.to_json()), indent=2))\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + '''C:/Program Files (x86)/Graphviz2.38/bin/'''\n",
    "\n",
    "!pip install graphviz\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optmizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=3e-4), loss=dice_coef_loss,\n",
    "                  metrics=[dice_coef, 'accuracy', precision, recall, f1score])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=2018)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=train_batch_size, epochs=epochs, verbose=1, shuffle=True,\n",
    "          callbacks=[csv_logger, model_checkpoint],validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
